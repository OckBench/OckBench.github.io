<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="OckBench: The first benchmark measuring decoding token efficiency alongside accuracy for LLM reasoning. Tokens are not to be multiplied without necessity.">
  <meta name="keywords" content="OckBench, LLM, Token Efficiency, Reasoning Benchmark, Ockham's Razor, Decoding Efficiency">
  <title>OckBench: Tokens are Not to Be Multiplied without Necessity | NeurIPS 2025 Workshop on Efficient Reasoning</title>

  <link rel="icon" href="./static/images/ockbench-logo.svg">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <script src="./static/data/leaderboard_data.js" defer></script>

  <style>
    .no-sort {
        cursor: default;
        pointer-events: none;
        background-image: none !important;
    }
    .ockbench {
      font-weight: bold;
      color: #2563eb;
    }
    .metric-card {
      background: #f8fafc;
      border: 2px solid #e2e8f0;
      border-radius: 8px;
      padding: 20px;
      margin: 10px;
      text-align: center;
    }
    .metric-title {
      font-size: 14px;
      color: #64748b;
      margin-bottom: 8px;
    }
    .metric-value {
      font-size: 32px;
      font-weight: bold;
      color: #2563eb;
    }
  </style>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/topics/llm-benchmark">
            <b>LLM Benchmarks</b>
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/ockbench-logo.svg" style="width:1.6em;vertical-align: middle" alt="Logo"/>
            <span class="ockbench" style="vertical-align: middle">OckBench</span>
          </h1>
          <h2 class="subtitle is-3 publication-subtitle" style="margin-bottom: 20px;">
            Tokens are Not to Be Multiplied without Necessity
          </h2>
          
          <div class="is-size-5 publication-authors" style="margin: 20px auto;">
            <span class="author-block">
              <a href="#">Zheng Du</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="#">Hao Kang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="#">Song Han</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="#">Tushar Krishna</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Ligeng Zhu</a><sup>3</sup>
            </span>
          </div>
          <div class="is-size-7" style="margin-top: -8px; margin-bottom: 6px;">
            <span><sup>*</sup>Equal contribution</span>
          </div>
          
          <div class="is-size-6 publication-authors" style="margin: 10px auto;">
            <span class="author-block" style="margin-right: 2em;"><sup>1</sup>Georgia Institute of Technology</span>
            <span class="author-block" style="margin-right: 2em;"><sup>2</sup>MIT</span>
            <span class="author-block"><sup>3</sup>NVIDIA</span>
          </div>
          
          <div class="is-size-5 publication-authors" style="width: 80%; margin: 20px auto;">
            <span class="author-block" style="font-size:20px">NeurIPS 2025 Workshop on Efficient Reasoning</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.05722" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/OckBench/OckBench" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <!-- Dataset Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ockbench/ockbench" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ü§ó</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <!-- Leaderboard Link -->
              <span class="link-block">
                <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">üèÜ</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p style="text-align: center; font-style: italic; margin-bottom: 20px;">
            "Entities must not be multiplied beyond necessity." ‚Äî The Principle of Ockham's Razor
          </p>
          <p>
            Large Language Models (LLMs) such as GPT-4, Claude 3, and Gemini have demonstrated remarkable capabilities in complex problem-solving, largely attributed to their advanced reasoning abilities. Techniques like Chain of Thought (CoT) prompting and self-reflection have become central to this success, enabling models to perform step-by-step deductions for tasks requiring deep knowledge and logical rigor. However, as the industry increasingly emphasizes this "long decoding" mode, the computational cost associated with these reasoning processes has grown significantly.
          </p>
          <p>
            While LLM evaluation and comparison have become increasingly important, most evaluations focus primarily on <strong>accuracy</strong> while the <strong>efficiency of generation</strong> is less discussed. For example, HELM, LM-Eval, and the LMSYS Chatbot Arena rank models almost entirely on task accuracy. Yet in real systems, the difference between generating 10K tokens vs 100K tokens is non-trivial in latency, cost, and energy.
          </p>
          <p>
            We introduce <strong>OckBench</strong>, the first model-agnostic, hardware-agnostic benchmark that jointly measures <strong>accuracy and decoding token count</strong> for reasoning and coding tasks. Our key contributions include:
          </p>
          <ul style="text-align: left; display: inline-block;">
            <li><strong>Model-Agnostic Efficiency Metric:</strong> We formalize decoding token count as an intrinsic, hardware- and system-independent efficiency metric</li>
            <li><strong>Efficiency-Accuracy Aware Benchmark:</strong> The first unified benchmark specifically designed to evaluate LLM reasoning efficiency by measuring token consumption alongside accuracy</li>
            <li><strong>Empirical Trade-offs:</strong> We conduct experiments across multiple open- and closed-source models, revealing substantial practical trade-offs on the accuracy‚Äìefficiency Pareto frontier</li>
          </ul>
          <p>
            Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption. For instance, among commercial models, one high-accuracy model required over 2√ó the tokens of another to achieve similar accuracy. This reveals that efficiency variance is a neglected but significant axis of differentiation in LLM evaluation.
          </p>
          
          <!-- Performance Comparison Figures -->
          <div class="columns is-centered is-multiline" style="margin-top: 40px;">
            <div class="column is-half">
              <figure style="text-align: center;">
                <img src="static/images/model_performance_comparison.png" alt="Model Performance Comparison across all evaluated models showing accuracy vs token efficiency trade-offs" style="width: 100%; height: auto; border: 1px solid #e2e8f0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <figcaption style="text-align: center; margin-top: 15px; font-size: 14px; color: #dc2626; font-weight: 600;">
                  Commercial models with similar performance level but spent token budget differently
                </figcaption>
              </figure>
            </div>
            
            <div class="column is-half">
              <figure style="text-align: center;">
                <img src="static/images/performance_comparison_7b.png" alt="Performance comparison of 7B parameter models highlighting efficiency differences" style="width: 100%; height: auto; border: 1px solid #e2e8f0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <figcaption style="text-align: center; margin-top: 15px; font-size: 14px; color: #dc2626; font-weight: 600;">
                  Similar model size but takes drastically different time in reasoning tasks
                </figcaption>
              </figure>
            </div>
          </div>
          
          <!-- Additional Figure: Accuracy vs Tokens -->
          <div class="columns is-centered" style="margin-top: 30px;">
            <div class="column is-full">
              <figure style="text-align: center;">
                <img src="static/images/teaser.png" alt="Accuracy vs token consumption trade-off analysis across dense and refined reasoning models" style="width: 100%; max-width: 1200px; height: auto; border: 1px solid #e2e8f0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                <figcaption style="text-align: center; margin-top: 15px; font-size: 14px; color: #dc2626; font-weight: 600;">
                  Accuracy vs. token consumption trade-off analysis across dense and refined reasoning models, demonstrating the efficiency frontier for different model configurations
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
        <div class="content">
          <p class="mt-3">
            Performance of various LLMs on OckBench. Models are ranked by their reasoning efficiency, 
            which is computed as #Tokens / Accuracy (lower is better). Click on the tabs to switch between Math and Coding domains.
          </p>
          <!-- <p style="font-size: 14px; color: #64748b;">
            <strong>#Tokens:</strong> Average decoding token count &nbsp;&nbsp;
            <strong>Accuracy (%):</strong> Percentage of correctly solved problems &nbsp;&nbsp;
            <strong>Reasoning Efficiency:</strong> #Tokens / Accuracy (lower is better)
          </p> -->
        </div>

        <!-- Domain Tabs -->
        <div class="tabs is-centered is-boxed" style="margin-top: 20px;">
          <ul>
            <li class="is-active" data-tab="math">
              <a>
                <span class="icon is-small"><i class="fas fa-calculator"></i></span>
                <span>OckBench-Math</span>
              </a>
            </li>
            <li data-tab="coding">
              <a>
                <span class="icon is-small"><i class="fas fa-code"></i></span>
                <span>OckBench-Coding</span>
              </a>
            </li>
          </ul>
        </div>

        <!-- Math Leaderboard -->
        <div id="math-table" class="leaderboard-table">
          <table class="js-sort-table" id="results-math" style="margin-left: auto; margin-right: auto; margin-top: 20px;">
            <thead>
              <tr>
                <th class="no-sort" style="vertical-align: middle; width: 50px;"><strong>#</strong></th>
                <th class="no-sort" style="vertical-align: middle; width: 250px;"><strong>Model</strong></th>
                <th class="no-sort" style="vertical-align: middle; width: 100px;"><strong>Category</strong></th>
                <th data-js-sort-colNum="3" style="vertical-align: middle; width: 120px;"><strong>#Tokens</strong></th>
                <th data-js-sort-colNum="4" style="vertical-align: middle; width: 120px;"><strong>Accuracy (%)</strong></th>
                <th data-js-sort-colNum="5" style="vertical-align: middle; width: 150px;"><strong>Reasoning Efficiency</strong></th>
              </tr>
            </thead>
            <tbody>
              <!-- <tr><td>1</td><td style="text-align: left; padding-left: 10px;"><b>Sky-T1-7B</b><br><span style="font-size: 12px; color: #858383;">Skywork</span></td><td>Open Source</td><td>531.55</td><td>62.00</td><td>448.4</td></tr> -->
              <tr><td>2</td><td style="text-align: left; padding-left: 10px;"><b>GPT-4.1</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>1,349.88</td><td>59.00</td><td>152.1</td></tr>
              <tr><td>3</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-30B-A3B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>4,139.33</td><td>83.00</td><td>138.1</td></tr>
              <tr><td>4</td><td style="text-align: left; padding-left: 10px;"><b>GPT-5</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>3,641.89</td><td>78.00</td><td>130.3</td></tr>
              <tr><td>5</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-235B-A22B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>4,774.40</td><td>81.50</td><td>113.4</td></tr>
              <tr><td>6</td><td style="text-align: left; padding-left: 10px;"><b>Gemini-3 Pro</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>6,190.85</td><td>88.00</td><td>110.1</td></tr>
              <tr><td>7</td><td style="text-align: left; padding-left: 10px;"><b>GPT-4o</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>570.17</td><td>38.50</td><td>100.1</td></tr>
              <tr><td>8</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-14B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>5,084.48</td><td>79.00</td><td>97.0</td></tr>
              <tr><td>9</td><td style="text-align: left; padding-left: 10px;"><b>GPT-o3</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>3,513.90</td><td>69.50</td><td>95.5</td></tr>
              <tr><td>10</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-32B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>4,797.89</td><td>76.50</td><td>93.3</td></tr>
              <tr><td>11</td><td style="text-align: left; padding-left: 10px;"><b>Gemini-2.5 Pro</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>6,392.96</td><td>83.00</td><td>89.4</td></tr>
              <tr><td>12</td><td style="text-align: left; padding-left: 10px;"><b>AReaL-boba-2-32B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open Source</td><td>5,387.96</td><td>78.00</td><td>88.1</td></tr>
              <tr><td>13</td><td style="text-align: left; padding-left: 10px;"><b>AReal-boba-2-14B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open Source</td><td>6,154.75</td><td>79.50</td><td>81.6</td></tr>
              <tr><td>14</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-8B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>6,302.89</td><td>80.00</td><td>81.2</td></tr>
              <tr><td>15</td><td style="text-align: left; padding-left: 10px;"><b>AReal-boba-2-8B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open Source</td><td>7,026.88</td><td>79.00</td><td>70.2</td></tr>
              <tr><td>16</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-4B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>6,478.45</td><td>72.00</td><td>57.6</td></tr>
              <tr><td>17</td><td style="text-align: left; padding-left: 10px;"><b>Gemini-2.5 Flash</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>7,984.58</td><td>74.50</td><td>51.8</td></tr>
              <tr><td>18</td><td style="text-align: left; padding-left: 10px;"><b>AceReason-Nemotron-7B</b><br><span style="font-size: 12px; color: #858383;">NVIDIA</span></td><td>Open Source</td><td>4,022.79</td><td>57.00</td><td>46.0</td></tr>
              <tr><td>19</td><td style="text-align: left; padding-left: 10px;"><b>AceReason-Nemotron-14B</b><br><span style="font-size: 12px; color: #858383;">NVIDIA</span></td><td>Open Source</td><td>4,147.26</td><td>56.50</td><td>43.5</td></tr>
              <tr><td>20</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-235B-A22B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>980.29</td><td>34.50</td><td>41.9</td></tr>
              <tr><td>21</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-14B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>2,421.85</td><td>43.50</td><td>34.0</td></tr>
              <tr><td>22</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-32B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>1,713.91</td><td>36.00</td><td>27.2</td></tr>
              <!-- <tr><td>23</td><td style="text-align: left; padding-left: 10px;"><b>Sky-T1-mini</b><br><span style="font-size: 12px; color: #858383;">Skywork</span></td><td>Open Source</td><td>8,266.21</td><td>49.00</td><td>14.2</td></tr> -->
              <tr><td>24</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-8B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>2,081.88</td><td>29.00</td><td>11.7</td></tr>
              <tr><td>25</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-4B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>2,262.83</td><td>27.50</td><td>9.2</td></tr>
              <tr><td>26</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-30B-A3B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open Source</td><td>1,852.56</td><td>25.00</td><td>8.4</td></tr>
            </tbody>
          </table>
        </div>

        <!-- Coding Leaderboard -->
        <div id="coding-table" class="leaderboard-table" style="display: none;">
          <table class="js-sort-table" id="results-coding" style="margin-left: auto; margin-right: auto; margin-top: 20px;">
            <thead>
              <tr>
                <th class="no-sort" style="vertical-align: middle; width: 50px;"><strong>#</strong></th>
                <th class="no-sort" style="vertical-align: middle; width: 250px;"><strong>Model</strong></th>
                <th class="no-sort" style="vertical-align: middle; width: 100px;"><strong>Category</strong></th>
                <th data-js-sort-colNum="3" style="vertical-align: middle; width: 120px;"><strong>#Tokens</strong></th>
                <th data-js-sort-colNum="4" style="vertical-align: middle; width: 120px;"><strong>Accuracy (%)</strong></th>
                <th data-js-sort-colNum="5" style="vertical-align: middle; width: 150px;"><strong>Reasoning Efficiency</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr><td>1</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-235B-A22B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>519.31</td><td>61.00</td><td>851.3</td></tr>
              <tr><td>2</td><td style="text-align: left; padding-left: 10px;"><b>gpt-4.1</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>639.80</td><td>74.50</td><td>858.1</td></tr>
              <tr><td>3</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-32B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>493.23</td><td>53.50</td><td>921.9</td></tr>
              <tr><td>4</td><td style="text-align: left; padding-left: 10px;"><b>gpt-4o</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>573.54</td><td>61.50</td><td>932.6</td></tr>
              <tr><td>5</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-30B-A3B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>631.42</td><td>54.00</td><td>1,169.3</td></tr>
              <tr><td>6</td><td style="text-align: left; padding-left: 10px;"><b>gpt-5</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>1,584.41</td><td>96.50</td><td>1,641.8</td></tr>
              <tr><td>7</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-14B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>990.27</td><td>51.00</td><td>1,941.7</td></tr>
              <tr><td>8</td><td style="text-align: left; padding-left: 10px;"><b>Qwen/Qwen3-4B</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>893.80</td><td>39.50</td><td>2,263.8</td></tr>
              <tr><td>9</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-4B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>893.80</td><td>39.50</td><td>2,263.8</td></tr>
              <tr><td>10</td><td style="text-align: left; padding-left: 10px;"><b>o3</b><br><span style="font-size: 12px; color: #858383;">OpenAI</span></td><td>Commercial</td><td>2,335.24</td><td>93.50</td><td>2,497.6</td></tr>
              <tr><td>11</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-8B (non-thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>1,073.26</td><td>41.50</td><td>2,586.2</td></tr>
              <!-- <tr><td>12</td><td style="text-align: left; padding-left: 10px;"><b>Sky-T1-7B</b><br><span style="font-size: 12px; color: #858383;">Skywork</span></td><td>Open-Source</td><td>1,014.21</td><td>25.50</td><td>3,988.3</td></tr> -->
              <tr><td>13</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-30B-A3B-Thinking-2507</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>4,846.07</td><td>85.50</td><td>5,671.6</td></tr>
              <tr><td>14</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-235B-A22B-Thinking-2507</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>5,927.84</td><td>92.00</td><td>6,443.3</td></tr>
              <tr><td>15</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-32B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>6,135.86</td><td>85.50</td><td>7,179.0</td></tr>
              <tr><td>16</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-235B-A22B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>6,545.30</td><td>85.00</td><td>7,700.4</td></tr>
              <tr><td>17</td><td style="text-align: left; padding-left: 10px;"><b>gemini-3-pro-preview</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>7,523.07</td><td>95.00</td><td>7,929.5</td></tr>
              <tr><td>18</td><td style="text-align: left; padding-left: 10px;"><b>AReaL-boba-2-32B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open-Source</td><td>6,901.34</td><td>85.00</td><td>8,119.2</td></tr>
              <tr><td>19</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-30B-A3B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>6,412.52</td><td>76.50</td><td>8,388.9</td></tr>
              <tr><td>20</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-14B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>6,850.90</td><td>77.00</td><td>8,897.3</td></tr>
              <tr><td>21</td><td style="text-align: left; padding-left: 10px;"><b>AReal-boba-2-14B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open-Source</td><td>7,519.10</td><td>79.50</td><td>9,461.4</td></tr>
              <tr><td>22</td><td style="text-align: left; padding-left: 10px;"><b>AceReason-Nemotron-7B</b><br><span style="font-size: 12px; color: #858383;">NVIDIA</span></td><td>Open-Source</td><td>7,082.74</td><td>67.50</td><td>10,419.6</td></tr>
              <tr><td>23</td><td style="text-align: left; padding-left: 10px;"><b>AceReason-Nemotron-14B</b><br><span style="font-size: 12px; color: #858383;">NVIDIA</span></td><td>Open-Source</td><td>8,515.95</td><td>76.50</td><td>11,128.7</td></tr>
              <tr><td>24</td><td style="text-align: left; padding-left: 10px;"><b>gemini-2.5-flash</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>9,884.73</td><td>85.00</td><td>11,629.1</td></tr>
              <tr><td>25</td><td style="text-align: left; padding-left: 10px;"><b>gemini-2.5-pro</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>9,808.19</td><td>79.00</td><td>12,417.9</td></tr>
              <tr><td>26</td><td style="text-align: left; padding-left: 10px;"><b>AReal-boba-2-8B</b><br><span style="font-size: 12px; color: #858383;">inclusionAI</span></td><td>Open-Source</td><td>10,521.25</td><td>70.50</td><td>14,959.2</td></tr>
              <tr><td>27</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-8B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>10,637.60</td><td>65.50</td><td>16,251.0</td></tr>
              <tr><td>28</td><td style="text-align: left; padding-left: 10px;"><b>Qwen3-4B (thinking)</b><br><span style="font-size: 12px; color: #858383;">Alibaba</span></td><td>Open-Source</td><td>13,639.89</td><td>58.00</td><td>23,517.1</td></tr>
              <!-- <tr><td>29</td><td style="text-align: left; padding-left: 10px;"><b>Sky-T1-mini</b><br><span style="font-size: 12px; color: #858383;">Skywork</span></td><td>Open-Source</td><td>15,132.72</td><td>51.00</td><td>29,672.0</td></tr> -->
              <tr><td>30</td><td style="text-align: left; padding-left: 10px;"><b>gemini-2.5-flash-lite</b><br><span style="font-size: 12px; color: #858383;">Google</span></td><td>Commercial</td><td>54,042.28</td><td>40.50</td><td>133,435.3</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f8fafc;">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Benchmark Overview</h2>
        <div class="content">
          <p class="subtitle is-5">
            OckBench provides comprehensive evaluation across multiple dimensions
          </p>
        </div>

        <!-- Statistics Cards -->
        <div class="columns is-multiline is-centered" style="margin-top: 30px;">
          <div class="column is-one-quarter">
            <div class="metric-card">
              <div class="metric-title">Total Questions</div>
              <div class="metric-value">400</div>
            </div>
          </div>
          <div class="column is-one-quarter">
            <div class="metric-card">
              <div class="metric-title">Task Domains</div>
              <div class="metric-value">2</div>
            </div>
          </div>
          <div class="column is-one-quarter">
            <div class="metric-card">
              <div class="metric-title">Questions per Domain</div>
              <div class="metric-value">200</div>
            </div>
          </div>
          <div class="column is-one-quarter">
            <div class="metric-card">
              <div class="metric-title">Models Evaluated</div>
              <div class="metric-value">24</div>
            </div>
          </div>
        </div>

        <!-- Task Domains -->
        <div class="content has-text-left" style="margin-top: 40px; max-width: 800px; margin-left: auto; margin-right: auto;">
          <h3 class="title is-4 has-text-centered">Benchmark Composition</h3>
          <p style="text-align: justify;">
            OckBench is structured to test LLMs' reasoning efficiency across two complementary domains: mathematical problem solving and coding skills. To better expose token-efficiency differences, we select questions that exhibit high variance in decoding token usage among baseline models.
          </p>
          <ul style="margin-top: 20px;">
            <li><strong>Mathematics and Reasoning Tasks (200 questions):</strong> We adopt GSM8K, AIME24, and AIME25 as core reasoning benchmarks. We select the top 200 questions that exhibit high variance in decoding token usage among baseline models, ensuring the benchmark emphasizes efficiency contrast rather than merely ranking by accuracy.</li>
            <li><strong>Software Engineering Tasks (200 questions):</strong> For the coding domain, we build a lightweight variant of MBPP, supplemented by 200 carefully curated real-world coding problems. These coding tasks cover algorithmic challenges, code transformation, debugging, and small-scale project tasks.</li>
          </ul>
        </div>

        <!-- Evaluation Metrics -->
        <!-- <div class="content has-text-left" style="margin-top: 40px; max-width: 800px; margin-left: auto; margin-right: auto;">
          <h3 class="title is-4 has-text-centered">Evaluation Metrics</h3>
          <p style="text-align: justify; margin-bottom: 15px;">
            OckBench uses <strong>decoding token count</strong> as the core efficiency metric‚Äîa model- and hardware-agnostic measure that captures the intrinsic reasoning efficiency of models.
          </p>
          <ul>
            <li><strong>Decoding Token Count (#Tokens):</strong> Total number of tokens generated during reasoning (model-agnostic, hardware-agnostic)</li>
            <li><strong>Accuracy (%):</strong> Percentage of correctly solved problems</li>
            <li><strong>Reasoning Efficiency:</strong> Computed as #Tokens / Accuracy, measuring cost per unit of correctness</li>
          </ul>
          <p style="text-align: justify; margin-top: 15px; font-style: italic; color: #64748b;">
            By selecting questions with high variance in token consumption across models, OckBench reveals efficiency differences that traditional accuracy-only benchmarks miss.
          </p>
        </div> -->
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f8fafc;">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Example Tasks</h2>
        <div class="content">
          <p class="subtitle is-5">
            Sample problems from OckBench-Math and OckBench-Coding
          </p>
          <p style="font-size: 14px; color: #64748b;">
            These examples illustrate the types of problems where token efficiency varies significantly across models.
          </p>
        </div>

        <div class="columns is-multiline" style="margin-top: 30px;">
          <!-- Math Example 1 -->
          <div class="column is-half">
            <div class="box" style="text-align: left; min-height: 220px;">
              <h4 class="title is-5" style="color: #2563eb;"><i class="fas fa-calculator"></i> Math Problem (GSM8K)</h4>
              <p><strong>Question:</strong> A store sells notebooks for $3 each. If you buy more than 10, you get a 20% discount on the total price. How much would it cost to buy 15 notebooks?</p>
              <p style="margin-top: 10px; font-size: 14px; color: #64748b;">
                <strong>Domain:</strong> Mathematics &nbsp;|&nbsp; <strong>Source:</strong> GSM8K
              </p>
              <p style="margin-top: 10px; font-size: 13px; color: #2563eb; font-style: italic;">
                Token variance: Some models use 200 tokens, others use 2,000+ for the same answer.
              </p>
            </div>
          </div>

          <!-- Math Example 2 -->
          <div class="column is-half">
            <div class="box" style="text-align: left; min-height: 220px;">
              <h4 class="title is-5" style="color: #2563eb;"><i class="fas fa-calculator"></i> Math Problem (AIME)</h4>
              <p><strong>Question:</strong> Find the number of ordered pairs (a,b) of integers such that a¬≤ + b¬≤ = 2024 and both a and b are positive.</p>
              <p style="margin-top: 10px; font-size: 14px; color: #64748b;">
                <strong>Domain:</strong> Mathematics &nbsp;|&nbsp; <strong>Source:</strong> AIME 2024
              </p>
              <p style="margin-top: 10px; font-size: 13px; color: #2563eb; font-style: italic;">
                Token variance: High variance across models due to different reasoning approaches.
              </p>
            </div>
          </div>

          <!-- Coding Example 1 -->
          <div class="column is-half">
            <div class="box" style="text-align: left; min-height: 220px;">
              <h4 class="title is-5" style="color: #2563eb;"><i class="fas fa-code"></i> Coding Problem (MBPP)</h4>
              <p><strong>Task:</strong> Write a function to find the longest common subsequence of two strings. For example, lcs("ABCDGH", "AEDFHR") should return 3 (the LCS is "ADH").</p>
              <p style="margin-top: 10px; font-size: 14px; color: #64748b;">
                <strong>Domain:</strong> Coding &nbsp;|&nbsp; <strong>Source:</strong> MBPP variant
              </p>
              <p style="margin-top: 10px; font-size: 13px; color: #2563eb; font-style: italic;">
                Token variance: Efficient models write concise code with brief explanations.
              </p>
            </div>
          </div>

          <!-- Future Work -->
          <div class="column is-half">
            <div class="box" style="text-align: left; min-height: 220px; border: 2px dashed #cbd5e1; background-color: #f8fafc;">
              <h4 class="title is-5" style="color: #64748b;"><i class="fas fa-flask"></i> Future Work: More Domains</h4>
              <p style="color: #64748b;"><strong>Planned Extension:</strong> We plan to extend OckBench to additional domains such as algorithmic challenges, debugging tasks, and code transformation problems to provide more comprehensive token efficiency evaluation.</p>
              <p style="margin-top: 10px; font-size: 14px; color: #94a3b8;">
                <strong>Status:</strong> Coming in next version &nbsp;|&nbsp; <strong>Focus:</strong> Broader coverage
              </p>
              <p style="margin-top: 10px; font-size: 13px; color: #94a3b8; font-style: italic;">
                Stay tuned for expanded benchmark coverage across more reasoning domains!
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Findings</h2>
        <div class="content has-text-centered">
          <div style="max-width: 800px; margin: 0 auto; text-align: left;">
            <ul>
              <li><strong>2√ó token variance among top models:</strong> Gemini-2.5 Pro used 2√ó more tokens than GPT-5 for similar accuracy (5,198 vs 2,336 tokens).</li>
              <li><strong>GPT-4o is most token-efficient:</strong> Best reasoning efficiency of 14.1 (math) and 12.9 (coding) despite lower accuracy.</li>
              <li><strong>"Thinking" modes costly:</strong> Qwen3-14B thinking used 2.7√ó more tokens than non-thinking (8,190 vs 3,010) without proportional gains.</li>
              <li><strong>Commercial models lead:</strong> 60.8% average accuracy vs 35.3% for open-source on math tasks.</li>
              <li><strong>10-18√ó efficiency differences:</strong> Models with comparable accuracy differ wildly in token consumption.</li>
              <li><strong>Size doesn't guarantee efficiency:</strong> Smaller models sometimes use more tokens than larger ones for the same tasks.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f8fafc;">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Citation</h2>
        <div class="content">
          <p class="subtitle is-5">
            If you find OckBench useful for your research, please cite our work
          </p>
        </div>
        <div class="box" style="background-color: #ffffff; text-align: left; max-width: 700px; margin: 20px auto;">
          <pre style="background-color: #f8fafc; padding: 20px; border-radius: 5px; overflow-x: auto; margin: 0;"><code>@inproceedings{du2025ockbench,
  author    = {Du, Zheng and Kang, Hao and Zhu, Ligeng and Han, Song and Krishna, Tushar},
  title     = {OckBench: Tokens are Not to Be Multiplied without Necessity},
  booktitle = {NeurIPS 2025 Workshop on Efficient Reasoning},
  year      = {2025}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>OckBench</strong> - Evaluating Accuracy and Efficiency in LLM Reasoning
      </p>
      <p>
        NeurIPS 2025 Workshop on Efficient Reasoning
      </p>
      <p style="font-size: 14px; color: #64748b; margin-top: 20px;">
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </div>
</footer>

<script>
  // Navbar burger toggle
  document.addEventListener('DOMContentLoaded', () => {
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    if ($navbarBurgers.length > 0) {
      $navbarBurgers.forEach( el => {
        el.addEventListener('click', () => {
          const target = el.dataset.target;
          const $target = document.getElementById(target);
          el.classList.toggle('is-active');
          $target.classList.toggle('is-active');
        });
      });
    }
  });

  // Tab switching for leaderboard
  document.addEventListener('DOMContentLoaded', () => {
    const tabs = document.querySelectorAll('.tabs li');
    const mathTable = document.getElementById('math-table');
    const codingTable = document.getElementById('coding-table');
    
    tabs.forEach(tab => {
      tab.addEventListener('click', (e) => {
        e.preventDefault();
        
        // Remove active class from all tabs
        tabs.forEach(t => t.classList.remove('is-active'));
        
        // Add active class to clicked tab
        tab.classList.add('is-active');
        
        // Show/hide tables
        const tabType = tab.getAttribute('data-tab');
        if (tabType === 'math') {
          mathTable.style.display = 'block';
          codingTable.style.display = 'none';
        } else if (tabType === 'coding') {
          mathTable.style.display = 'none';
          codingTable.style.display = 'block';
        }
      });
    });
  });

  // Update row numbers on sort for both tables
  document.addEventListener("DOMContentLoaded", function() {
    updateRowNumbers('results-math');
    updateRowNumbers('results-coding');
    
    const mathTable = document.querySelector("#results-math");
    const codingTable = document.querySelector("#results-coding");
    
    if (mathTable) {
      mathTable.addEventListener("click", function() {
        setTimeout(() => updateRowNumbers('results-math'), 100);
      });
    }
    
    if (codingTable) {
      codingTable.addEventListener("click", function() {
        setTimeout(() => updateRowNumbers('results-coding'), 100);
      });
    }
  });

  function updateRowNumbers(tableId) {
    const rows = document.querySelectorAll(`#${tableId} tbody tr`);
    rows.forEach((row, index) => {
      row.querySelector("td").innerText = index + 1;
    });
  }
</script>

</body>
</html>

